{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "5mb577_FNLpS"
      },
      "source": [
        "# NLTK & processing a text file\n",
        "\n",
        "Download & open this file in Google Colabs! \n",
        "\n",
        "https://colab.research.google.com/\n",
        "\n",
        "This is a tutorial & introduction to NLTK, Natural Language Toolkit"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hb7TpvoBNLpX"
      },
      "source": [
        "## Using NLTK\n",
        "\n",
        "* NLTK ([Natural Language Toolkit](http://www.nltk.org/)) is an external library; you must import it first using an import statement. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "wZYPo1FGNLpX"
      },
      "outputs": [],
      "source": [
        "import nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGHLLGIlNLpZ"
      },
      "source": [
        "* Let's first download some data files. If we know the package or data files we want to import, we can pass that as an argument to the `nltk.download()` function. \n",
        "\n",
        "If not, we can interactively browse the files we want to download:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "8P4FvWeiNLpZ"
      },
      "outputs": [],
      "source": [
        "nltk.download()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's download the `popular` files, which will give us some nice things to work with.\n",
        "\n",
        "Let's use the function below to tokenize a sentence that we choose:\n",
        "\n"
      ],
      "metadata": {
        "id": "96NfT0VJUqXv"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9jtQtLmPNLpa"
      },
      "outputs": [],
      "source": [
        "# Tokenizing function: turns a text (a single string) into a list of word & symbol tokens\n",
        "greet = \"\"\n",
        "nltk.word_tokenize(greet)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also look at just what happens when we use the word_tokenize module that's built in to NLTK by calling the `help()` function on it:"
      ],
      "metadata": {
        "id": "flaypW35WnQm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "avGRX8gQNLpb"
      },
      "outputs": [],
      "source": [
        "help(nltk.word_tokenize)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rtk7qNC0NLpc"
      },
      "outputs": [],
      "source": [
        "sentence = \"\"\n",
        "nltk.word_tokenize(sentence)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZXHjyoYANLpd"
      },
      "source": [
        "`nltk.FreqDist()` is is another useful NLTK function. \n",
        "\n",
        "It builds a frequency count dictionary from a list. \n",
        "\n",
        "Let's start with just a sentence, so we can see a manageable output:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A8a6hdpaNLpd"
      },
      "outputs": [],
      "source": [
        "\n",
        "rose = 'A rose is a rose is a rose is a rose.'\n",
        "tokens = nltk.word_tokenize(rose.lower())\n",
        "print(tokens)\n",
        "print(\"Length of sentence: \", len(tokens))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iQADzRkyNLpd"
      },
      "outputs": [],
      "source": [
        "freq = nltk.FreqDist(tokens)\n",
        "freq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also use the most_common() method to look at the most common elements in our frequency distribution. \n",
        "\n",
        "This method takes an optional argument -- the default (with no argument) is to display all. But you can specify how many you'd like to see by putting in a number here:\n",
        "\n",
        "`freq.most_common(2)`"
      ],
      "metadata": {
        "id": "J2L23igBYvR0"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GtZ8KCL9NLpe"
      },
      "outputs": [],
      "source": [
        "#help(nltk.FreqDist.most_common)\n",
        "freq.most_common()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also just look at a single token's frequency by looking up that key in our frequency dictionary (ie: enclosing that token in square brackets):"
      ],
      "metadata": {
        "id": "E7Gm58CEZXaT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ReDrFjgQNLpe"
      },
      "outputs": [],
      "source": [
        "freq['rose']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Taking the length of the frequency table will get us the number of types!"
      ],
      "metadata": {
        "id": "b3n37p2hZf-T"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5cSY7oVzNLpf"
      },
      "outputs": [],
      "source": [
        "print('the number of types is : ', len(freq))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also of course get the list of keys (aka, our tokens) in this dictionary."
      ],
      "metadata": {
        "id": "2WqiGIaDZ0QJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YztsyF0_NLpf"
      },
      "outputs": [],
      "source": [
        "freq.keys()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pUVZkCfGNLpg"
      },
      "source": [
        "## Processing a single text file\n",
        "\n",
        "### Reading in a text file\n",
        "* `open(filename).read()` opens a text file and reads in the content as a *single continuous string*. \n",
        "\n",
        "We will need to upload the texts we want to be working with into this colab. I've included some in our Week 7 repository (3 chapters of Moby Dick)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "379YSnqwNLpg"
      },
      "outputs": [],
      "source": [
        "myfile = '/content/three-chaps-mobyDick.txt'  \n",
        "mobyDick = open(myfile).read()\n",
        "print(mobyDick)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's make sure we've got things loaded properly...\n",
        "\n",
        "Getting the length will tell us how many characters we have total (since this is read in as a string):"
      ],
      "metadata": {
        "id": "tpix-74pb_rm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TMkrYpIpNLpg"
      },
      "outputs": [],
      "source": [
        "len(mobyDick)     # Number of characters in text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We know the word 'harpooneer' occurs in this text, let's make sure we can find it:"
      ],
      "metadata": {
        "id": "FuUrNuO-cSZx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HrZAEbooNLph"
      },
      "outputs": [],
      "source": [
        "'harpooneer' in mobyDick  # phrase as a substring. try \"harpooner\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h-LhKNDdNLph"
      },
      "source": [
        "### Tokenize text, compile frequency count\n",
        "\n",
        "NLTK allows us to do a lot of the manual work that we did last week of compiling a dictionary of our input text and their frequency counts in a few quick steps:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ank1PrE5NLpi"
      },
      "outputs": [],
      "source": [
        "# Turn off/on pretty printing (prints too many lines)\n",
        "%pprint    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B5WXSAjoNLpi"
      },
      "outputs": [],
      "source": [
        "# Lowercase & Tokenize text\n",
        "nltk.word_tokenize(mobyDick.lower())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZtRFfCnWNLpi"
      },
      "outputs": [],
      "source": [
        "mtokens = nltk.word_tokenize(mobyDick.lower())\n",
        "len(mtokens)     # Number of words in text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iJEPzLwkNLpi"
      },
      "outputs": [],
      "source": [
        "# Build a dictionary of frequency count\n",
        "mfreq = nltk.FreqDist(mtokens)\n",
        "mfreq['the']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6Lw4qwBXNLpj"
      },
      "outputs": [],
      "source": [
        "'harpooneer' in mfreq"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Once we have the dictionary, we can get the counts of unique words in our text (aka, types):"
      ],
      "metadata": {
        "id": "39rsbQEDc-Lm"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xz8ADP70NLpj"
      },
      "outputs": [],
      "source": [
        "len(mfreq)      # Number of unique words in text"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Take a look at the 30 most common words:"
      ],
      "metadata": {
        "id": "XSXndleOdKuM"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "scrolled": true,
        "id": "J8ugaHIzNLpj"
      },
      "outputs": [],
      "source": [
        "mfreq.most_common(30)     # 30 most common words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### What else can we do?\n",
        "\n",
        "If we use dir() on this dictionary object that we've created that holds the Moby Dick keys and frequencies, we'll get a list of things we can do. We have to scroll past some other things, but here at the end, we can see a bunch of methods and functions that we can call on this dictionary of data:"
      ],
      "metadata": {
        "id": "9NHRiHT5dOfV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AxaJO5cUNLpj"
      },
      "outputs": [],
      "source": [
        "# dir() prints out all functions defined on the type of object. \n",
        "dir(mfreq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mxU3-D79NLpk"
      },
      "outputs": [],
      "source": [
        "# Hmm. Wonder what .freq does... let's find out. \n",
        "help(mfreq.freq)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cInY5lUuNLpk"
      },
      "outputs": [],
      "source": [
        "mfreq.freq('the')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also look at `plot` , and hapax legomena"
      ],
      "metadata": {
        "id": "Vh4ezKtEeh5X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(mfreq.plot)"
      ],
      "metadata": {
        "id": "wkPlP7ZQekio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "mfreq.plot()"
      ],
      "metadata": {
        "id": "Cal-OvxzeoZy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xw7MQrR5NLpl"
      },
      "outputs": [],
      "source": [
        "len(mfreq.hapaxes())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fCFAnQRrNLpl"
      },
      "source": [
        "### Average sentence length, frequency of long words\n",
        "\n",
        "This is another common text analysis method that we want to look at.\n",
        "\n",
        "First, let's count the number of sentences. We'll do this crudely, and assume that all sentences end with only 3 punctuation elements:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ybnCQaJDNLpl"
      },
      "outputs": [],
      "source": [
        "sentcount = mfreq['.'] + mfreq['?'] + mfreq['!']  # Assuming every sentence ends with ., ! or \n",
        "print(sentcount)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3aPU6nxeNLpm"
      },
      "outputs": [],
      "source": [
        "# Tokens include symbols and punctuation. First 50 tokens:\n",
        "mtokens[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We want to make sure that we're not counting punctuation in our sentence-length counts, so let's create a new list where we only include words if they're alpha-numeric:"
      ],
      "metadata": {
        "id": "eoBiMEO-iU1u"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jfJ7gEYPNLpm"
      },
      "outputs": [],
      "source": [
        "mtokens_nosym = [token for token in mtokens if token.isalnum()]    # A list comprehensoion! Make sure they are alpha-numeric tokens only\n",
        "len(mtokens_nosym)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Is this ok?"
      ],
      "metadata": {
        "id": "6lqmwLzqi5UB"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HHP__GXsNLpm"
      },
      "outputs": [],
      "source": [
        "# Try \"n't\", \"4th\", \".\"\n",
        "\".\".isalnum()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gDwYCD7ANLpm"
      },
      "outputs": [],
      "source": [
        "# First 50 tokens, alpha-numeric tokens only: \n",
        "mtokens_nosym[:50]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Divide the number of word tokens by the count of how many sentences we have to get our average sentence length"
      ],
      "metadata": {
        "id": "35Wi3M2ikUAf"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CzSvj4lENLpn"
      },
      "outputs": [],
      "source": [
        "len(mtokens_nosym)/sentcount     # Average sentence length in number of words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Get a list of all the really long words, which tend to be less frequent. (They also tend to be the words that can help determine authorship!)"
      ],
      "metadata": {
        "id": "iTNRgvgOkEmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ueuVwrYfNLpn"
      },
      "outputs": [],
      "source": [
        "[word for word in mfreq if len(word) >= 13]       # all 13+ character words"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MaMndYhQNLpn"
      },
      "outputs": [],
      "source": [
        "long = [word for word in mfreq if len(word) >= 13] \n",
        "# sort long alphabetically using sorted()\n",
        "for word in sorted(long) :\n",
        "    print(word, len(word), mfreq[word])               # long words tend to be less frequent"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Stemming & Lemmatization"
      ],
      "metadata": {
        "id": "vWuRUs1IkdmC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import these modules\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize"
      ],
      "metadata": {
        "id": "hfyL1jiFkdOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's import the Porter Stemmer"
      ],
      "metadata": {
        "id": "XNjK_fXck0Z2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ps = PorterStemmer()\n",
        " \n",
        "# choose some words to be stemmed\n",
        "words = [\"linguistics\", \"linguist\", \"linguistically\", \"is\", \"was\", \"are\", \"were\", \"am\", \"be\", \"being\"]\n",
        " \n",
        "for w in words:\n",
        "    print(w, \" : \", ps.stem(w))"
      ],
      "metadata": {
        "id": "q8tO-ir3kzie"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's also take a look at the difference here in terms of lemmatizing"
      ],
      "metadata": {
        "id": "z2thy4dAm8kF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem import WordNetLemmatizer\n",
        " \n",
        "lemmatizer = WordNetLemmatizer()\n"
      ],
      "metadata": {
        "id": "zLIfB7e9m8E1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for word in words:\n",
        "  print(word, \" : \", lemmatizer.lemmatize(word)) \n",
        "\n",
        "# a denotes adjective in \"pos\"\n",
        "#print(\"better :\", lemmatizer.lemmatize(\"better\", pos =\"a\"))"
      ],
      "metadata": {
        "id": "uycbD7qDnDrm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "So this is a little better, but not a ton -- let's take a look at the lemmatize() function to see what we can do with that:\n",
        "\n",
        "\n",
        "Reference: https://www.geeksforgeeks.org/python-lemmatization-with-nltk/"
      ],
      "metadata": {
        "id": "6Ub6Y4cNoLSO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "help(lemmatizer.lemmatize)"
      ],
      "metadata": {
        "id": "s8yXn1-qob2O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "wordsDict = {\"linguistics\": \"n\", \"linguist\": \"n\", \"linguistically\": \"a\", \"is\": \"v\", \"was\": \"v\", \"are\": \"v\", \"were\": \"v\", \"am\": \"v\", \"be\": \"v\", \"being\": \"v\"}\n",
        "\n",
        "for key in wordsDict:\n",
        "  print(key, \" : \", lemmatizer.lemmatize(key, pos = wordsDict[key])) "
      ],
      "metadata": {
        "id": "HwODf0EFniaC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ra1AvCU_oTAT"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.5.3"
    },
    "colab": {
      "name": "NLTK intro",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}